## 大模型RAPT(RAG,Agents,Prompt and Tuning)技术群第二次线上讨论会议通知

### 环节1：论文导读

#### 论文1: Retrieve Anything To Augment Large Language Models
- **论文链接**：[https://arxiv.org/abs/2310.07554](https://arxiv.org/abs/2310.07554)

#### 论文2：CodeFusion: A Pre-trained Diffusion Model for Code Generation
- **论文链接**：[https://arxiv.org/abs/2310.17680](https://arxiv.org/abs/2310.17680)

### 环节2：工程问题探讨

- #### 在搜索推荐场景中，RAG模型如何弥补查询与商品信息的语义差距？

- #### 探讨BGE中的LLM-Embedder

- #### 应对策略：当嵌入模型上下文长度不足时的优化调整

### 环节3：自由讨论

**会议时间**：
- **北京时间**：2023/11/04 09:00-12:00
- **美国西部时间（洛杉矶）**：2023/11/03 18:00-21:00
- **迪拜时间**：2023/11/04 05:00-08:00
- **伦敦时间**：2023/11/04 01:00-04:00
- **北美东部时间（纽约）**：2023/11/03 21:00-00:00

点击链接入会，或添加至会议列表：
https://voovmeeting.com/dm/n8j8gYVGo51j

**#VooVMeeting**：608-0865-7565


## 大语言模型RAPT（RAG，Agents，Prompt & Tuning）技术群第二次讨论会议记录

#### LLM的Embedder论文介绍了一种向量模型，实现了知识和内容的召回能力，并提出了多种数据训练和同质负样本采样技巧。还讨论了知识蒸馏的方法。

会议开始，进行论文导读。第一篇论文是关于LLM的Embedder，它是专门为LLM设计的一个向量模型，实现了知识的召回、工具的召回、视力的召回和内容回忆的能力，总共实现了四个方面、四个维度的功能。
该论文的主要技巧有两个，一个是使用多种数据进行训练，另一个是使用同质负样本进行采样，即同类数据的负样本采样。论文的训练数据包括QA数据、对话数据以及人工工具学习和生成等数据。这五种数据的格式是query passage，其中第一种是QA数据，第二种是对话数据，第三种是指令和工具描述的数据，第四种是指令数据，第五种是生成的文本数据。
在训练部分，论文使用了一种奖励函数，将候选数据和真实数据进行奖励，形成了一个奖励机制。对比学习的关键点在于在同类型的数据下进行学习，论文使用了同类其他数据作为培训本进行学习。此外，论文还讨论了如何进行知识蒸馏，使用KL散度来最小化两个数据的分布，相关的公式在文中给出。

#### 大型语言模型的检索增强需求的新方法：LLM的Embedder方法的优化训练方法取得了良好实验结果。

接下来的发言人介绍了一篇名为"retrieve anything to augment large language models"的论文。论文讨论了大型语言模型面临的挑战以及从外部世界获取帮助的重要性。
论文提出了一种新的方法，称为LLM的Embedder方法，该方法使用统一的嵌入模型来支持大型语言模型的检索增强需求，并通过优化训练方法获得了良好的实验结果。该方法在知识增强、掌上校文建模和指令跟随等方面改善了大型语言模型的性能。

#### 采用defusion原理进行代码生成的模型效果相当，包括编码器、解码器和分类器，并使用交叉注意力获取更长的上下文信息。训练部分使用损失函数进行模型训练，推理部分通过迭代编码和解码的方式生成预测代码。模型具有20个不同的参数量，在不同语言测试中选择了Python、Bash和Excel。

然后再仔细去看这篇文章的主要内容的话,它是用一个70M到75M的一个参数的模型去跟350M到175B的一个参数的模型是效果是相当的。它采用了defusion的原理去做了代码生成。
defusion主要有三个部分，一个是编码器将自然语言映射到连续表达表示中，然后用扩散模型去造，最后是将造嵌入提供给解码器。这是它的模型框架。代码生成的数据有三种差不多的类型。
在defusion领域和这个模型熟悉的人中，希望大家可以分享。
除了编码器、解码器之外，该模型在解码的过程中使用了交叉注意力。之前也有人用defusion做文本生成，不同的是这次用自注意力获得了更长的上下文信息。整个模型框架包括编码器、解码器和分类器。在训练部分，它使用了一个损失函数，将这三个部分的分布最小化，进行模型训练。
在推理部分，它通过迭代编码和解码的方式来生成预测代码，这种方法速度较慢。在一张图中，可以看到这个模型有20个不同的参数量，下面是对不同语言进行测试的混淆矩阵，选择了Python、Bash和Excel。

#### 没有使用C++和Java等著名语言，使用了1200个扩散步骤和128长度的代码。评估中使用了N-Gram度量、相似性度量和编辑距离度量，表现良好。模型缺陷是只测试了三种语言，对复杂语言性能较差。难以生成长代码片段且推理延迟大。对于R-LM方法中Diffusion的作用存在疑问。

这篇论文没有使用其他更著名的语言，比如C++和Java。有提到为什么没有使用这些语言。关于实验数据的参数，它使用了1200个扩散步骤，代码长度为128，可能是最大长度为128。
使用了AdmW的优化器，没有使用全种衰减，学习率为某个值。在评估方面，使用CodeBurst评估Python的性能。
评估时使用了三种度量方法，一种是N-Gram度量，一种是相似性度量，最后一种是编辑距离度量。在这三个维度上都表现得比较好。
这篇文章比较简短。最后总结了模型的一个缺陷，虽然它在上述方面表现很好，但只测试了三种语言，对于更复杂的语言性能会较差。另外发现生成更长的代码片段会更困难，因为对上下文的依赖和理解不够好。推理延迟也非常大，可能不太适合在生产环境中使用。
如果是Diffusion方向的专业，可以在这三个方向上进行研究。

这是我对主讲人发言的总结，辛苦了。我想提一个问题，我最近读了一篇论文，是几个机构合作的，他们使用的方法叫做R-LM。我回头给大家发一下论文链接。
我看了一下它的基本思想是用欧式距离来选择代码，并使用KNM方法选择最优的代码。与这篇论文有点类似。我们两边可以一起学习。

但是这篇论文我有一点不理解，它里面的Diffusion是做什么用的？我知道现在Diffusion在图片生成上很火，但是在这里做什么用呢？我们知道图片生成是将原有图片它一直转换成白噪声，然后再把白噪声变成图片中的规律。但代码中的defusion我不太懂，能否共享一下代码？

#### defusion是将信号加噪声变成白噪声再逆推回有信号的方法。参数量巨大，能力强大。

对于defusion的效果为什么好，我也不太理解。参数量非常大，可以达到75亿或7500万参数量，实现与ChatGBT相当的能力。关于defusion模式，群里有人了解吗？

大佬，你能讲一下defusion吗？
我稍微找个安静的地方。

defusion实际上是将信号不断加噪声变成白噪声，然后学习逆推回有信号。就像一个大黑盒子，你不用管具体如何学习，就像transformer一样将其学到。

文章看起来很神奇，之前以为defusion只是一个图形的东西，像文字离散通信性好像不太一样，但后来仔细想了一下，可能还是有相似之处。在defusion中，进入的图实际上是一个嵌入，然后通过解码器将其解码成图像，这个解码器叫做VAE。

#### 内部使用VAE，输入输出为embedding形式，学习过程使用了跨越注意力。对图像的深层错误和代码合并可能影响逻辑，扩散生成图像可能解决这些问题。

它内部使用了VAE，并且要求输入和输出都是embedding形式。对于embedding的具体含义，可能不是特别重要。
学习过程使用了跨越注意力（across attention），这是一个比较奇特的东西。他认为跨越注意力相当于解码的过程，类似于Unet。Unet不会直接将噪声输入进去，而是通过跨越注意力将其应用在噪声中。从图像的扩散过程来看，我们可以学好embedding，然后再解码得到想要的结果。具体是处理文字、图像还是声音都可以，因为扩散过程是一个常见的场景，不一定局限于图像。

我觉得这给我的启发很大。我以前觉得对图像来说，图片生成的错误其实也没有太大关系，因为我们人眼看起来可能忽略了。而且在代码中，可能两个片段比较相似，我们将其强行合并在一起，似乎会影响逻辑。因为代码稍微有点不同，代码就跑不了。但是我想到了，当我们使用扩散生成图像时，实际上那些图像Diffusion生成的人像是很逼真且有逻辑的。所以Duffsion生成代码是可用的。

#### Diffusion网络通过引入Cross Attention条件机制，能够在特定条件下进行图像推演。

它有两种推演方式，一种是条件推演方式，一种是无条件推演方式。根据观察，我认为代码的这部分是完全属于无条件推演的。
条件推演实际上就是加上了一些条件的概率。在学习过程中，它同时学习了无条件和有条件的情况，主要是让它学习到世界上的图像是什么样子的，哪些是合理的，哪些是不合理的。

它不关心学到是一只狗还是一只猫。但是这就带来了一个问题，如果我随意加上一个条件之后，无法确定推出的图像是什么。可能是一只猫，也可能是一只狗。所以后来他们引入了一种叫做Cross Attention的条件机制，相当于加上一个控制信号。现在给定一句话的嵌入，它会成为控制信号。现在给定一个图像，不断地添加造型，直到添加到白噪声。这样，当我进行图像推演时，Diffusion网络不仅知道如何推图，还知道在某个特定条件下推图。我浅显的理解就是这样，感觉它和Diffusion的condition部分有点类似。

#### 会议进行，讨论了BGE和数据量对效果的影响。分享了数据的数量和质量，以及筛选好的数据的方法。提到了BGE和BAL，对此不太了解。

昨天我和我的领导讨论了一下，他可能之所以效果比较好，是因为他的数据量比较大，并且筛选的质量也不错。不只是做领域链，而是适应了多种任务。关于领域链，从他们公布的情况来看，他们使用了1亿个中文token和2亿个英文token，数据量还是相当大的。在任务的自适应情况下，他们使用了多种数据。他们的数据量可能达到了一个比较好的数量和质量，所以才会有一个比较好的效果。

这是他们开源出来的数据，可以看到他这里的中文记录有1亿条，英文数据有2亿条。我到时候可以把链接发到会议上，这是最大规模的关联文本对数据。还可以看到他们的数据概况，有各种类型的数据，比如标题和段落，查询和文本，还有句子对句子，各种类型都有。他们在论文中也提到了如何筛选出好的数据，对他们来说认为是好的数据。有没有其他问题？我也不是很清楚。

#### 智源公司开发了用于检索的向量模型BGE，用于传递垂直领域和最新新闻的知识给大型模型，并实现多轮对话的召回能力。

在论文导读中提到了智源公司开发的向量模型，即BAAI generation embedding（BGE），用于检索。由于大型模型无法触及一些垂直领域和最新新闻的知识，需要向量模型进行召回并传递给大型模型进行分析。论文中提到LLM embedder具有四种功能：知识的召回、工具的召回、视力的召回和长文本的召回。具体实现方法是通过多任务的数据进行训练，实现多轮对话的召回能力。

#### 多任务学习的方法和训练过程对于学习效果有重要影响。

多任务学习，通过对不同任务的数据集进行召回和匹配，如对问句和段落的匹配、对指令和描述的匹配来进行学习。还提到了instruct的tuning，即通过不同的提示或指令进行区分。在模型retrieve阶段，可能无法明确识别要检索哪个领域的信息，因此可能需要进行检索。作者提到的方法都是基于静态的方式，如baseline、对比和inbounded。在训练过程中，进行了对比学习的训练和调用的定制化，训练完后的效果可能会优于静态的方法。同时，提到了动态选择一些数据题目，包括工具类的题目。

#### 数据集的选择，以及召回阶段使用小模型的优势

他们选了几个数据集，我看还有什么QA的这种，那么我对这几个数据上我做训练，那么再选一些是不是也是这几个数据的对比，那他肯定是比这种天然的这种支持之前均带大规模训练出来的这个要好。

如果是召回阶段的话，我们还是用一个小模型，就BERT参数量大小的小模型去做召回，大模型据说召回表达不太优于那个小，据说可以用这种BGE什么Chinese啊，或者英文媒体什么那些来召回。因为他把这几类领域的内容打平了，同时他的训练效果比较好，他更像是一个偏工程域，在进一步落地时可能效果更好，其他一些贡献可能没太理解。

补充一下，我觉得这位同学的理解跟我的理解是非常相近的。因为实际上，嗯，我也做工程嘛，这个encoder是可以继续训练的。为什么要找这个BGE，其实，我更感兴趣的是后面要有问的问题，就前面问的问题，那个确实是retriever就是个小的东西，不能太大了，其实就直接是大，他只是刚开始在RAG里面帮助你做这么一件事儿，就是帮助你找回最好的语料，然后让回头让LLM找到最合适的答案。所以他其实就是撞，你说你肯定是两个嘛，不管是QA啊，还是QQ啊，还是就是semantic啊，什么啊，verity啊，反正他就是尽量去你拿你的问题去撞到，觉得他语料里头哪些embedding，他就把这些全抽出来了，嗯，以前我也在纳闷，为什么那个叫做openai的那个好厉害啊，就是我们自己训好像训不过他。

#### 训练中文模型，要针对中文常见内容进行训练，可超过OpenAI的表现。数据配比已有开源资料。对于模型上下文不足可分段小结或准备特定结构策略。

有人提到在训练模型时使用了embedded encoder学习了大量的东西，但在中文场景下的表现可能有所欠缺。如果能够针对中文或中文常见内容进行训练，很可能能够超过OpenAI的表现。对于BAI使用了哪类数据进行训练以及如何平衡各任务的分配，还不清楚是否有开源的资料。有一个github的链接叫flag embedding，具体说明了如何微调和进行困难样本的学习。对于数据配比，之前看到的一个网站已经开源了3亿条中英文文本。

#### 对于模型上下文不足的应对策略

有一种方法是对文本进行分段，每个段落进行小结，虽然会丢失一些精度，但整体上仍然可以保持。另一种方法是针对文本的特定结构，如代码，可能需要准备相应的策略。

#### 介绍了多任务Retriever作为一种常用的实用工具方法，并讨论了Bert等模型在处理特定语义时的差异。

进入第三个环节——自由讨论。

关于之前提到的LM Embedder，把小模型作为工具使用的一种常用且实用的方法。然后提出了一个问题：对于像Bert这种区分两个语义的模型，逻辑关系可能比大模型差。举了个例子，如果两个语义都是对描述性的，比如一个是红红的水果，另一个是一个苹果，Bert可能处理得不够好，需要使用大模型。对于第一个问题，有人提到可能误解了，Bert和小模型对于这种红红的和苹果这两种语句的理解可能存在差距。

#### 会议讨论了实验的限制和模型训练的困难，提问者关注了相似性区分的问题和数据设计的挑战。

会议进行了论文讨论，期间有人提出了两个问题。第一个问题是对于实验的限制，提问者想知道作者在没有考虑到的情况下会如何处理。第二个问题是关于模型训练难度和相似性的讨论。提问者认为相似性区分在正面例子上比较容易，但在负面例子上比较困难。他举了一个具体的例子来说明，即如果两个句子很接近，但要让它们区分开来就需要更远的例子来支持。他还对于数据设计的难度表示了好奇，希望对于做嵌入式数据结构的机构能分享一下他们的思路。


#### 两位老师提到了表示逻辑性和困难样例的问题，我会测试一下。对于模型框架没有太多新颖之处。解决数据问题的方法可以构造数据或以知识的方式处理。现在我们回到搜索推荐RAG模型的议题。

两位老师提到了两个建议。第一个老师提到了是否可以更好地表示逻辑性，第二个老师提到了困难样例，即相似但意思完全不同的句子，是否可以做一个更好的表达。我回去会再测试一下。之前我只对比了BGE和其他向量模型，结果发现BGE的能力确实超过了OpenAI的A002和其他中文模型。

针对第二位老师提到的模型框架，我已经了解了，它的框架基本上就是传统的BERT框架，没有太多新颖之处。关于蒋老师提到的数据问题，我有一个猜测，就是这些数据是否可以通过构造来解决。我们在上次会议上讨论过这个问题，有一种方法是在实际工程应用中经过测试后，可以将这类例子单独处理，而不用整个语言模型来处理。还有一种方法是使用数据增强(编者注：Data Augmentation)。

#### 搜索推广模型RAG和语义差距讨论及训练方法。讨论了BGE的通用性和diffusion的训练数据量。 code t5文本编码方式讨论。

会议继续讨论搜索推广厂RUG模型和收广推应用。主要讨论了查询与商品的语义差距以及如何通过模型训练来弥补。其中提到了使用RAG解决语义差距的方法，以及之前讨论的BGE的通用性。参会人员认为可以再回到之前的两篇论文来继续讨论，并希望讨论diffusion和encoder的相关内容。其中，有人询问diffusion的训练数据量的大小，以及对代码进行训练时是分开训练还是混在一起。还讨论了使用code t5进行文本编码的方式。

#### LLM的Embedder是为LLM设计的向量模型，使用多种数据和同态负样本采样进行训练，同时使用奖励函数和对比学习。还介绍了使用KL散度进行知识蒸馏的方法。研究需要进一步探讨LLM的生成长文本细节和diffusion的应用。

他在论文中提到了LLM的Embedder，这是一个专门为LLM设计的向量模型。它实现了知识、工具、实例和回忆的招回四个维度。在训练过程中，主要使用了两个trick。其中一个是多种数据的训练，另一个是同类数据的同态负样本采样。训练数据包括QA数据、对话数据、指令和工具描述，以及生成的文本。在训练过程中，使用了奖励函数将候选数据和真实数据进行奖励。对比学习主要是在同类数据下进行。此外，还介绍了使用KL散度进行知识蒸馏的方法。然而，具体细节如何生成长文本并不清楚。另外，也提到了diffusion的用途，它在推演时限定了上下文的长度，并且无法容纳太多的信息。此外，还有可能已经有人尝试过使用diffusion来生成代码，但效果可能不太理想。对于LLM的独特之处，我们需要再仔细研究一下。

#### Diffusion方法结合交叉注意力用于学习条件格式的代码。使用45万个条件，通过数字和条件模板进行处理，类似于图像学习。

按理来说,Diffusion这个东西应该已经被人用来做了各种各样的研究。但是我没有看到相关的代码。有人尝试用Diffusion学习，但效果并不好，所以可能没有发表相关的论文。对于Diffusion的这一段代码，我还没有完全理解它的原理。但我猜想它应该使用了交叉注意力这一常用方法。对于Diffusion的使用，可能需要很多东西才能完整理解，因为Transform和Diffusion都属于数据要求较高的方法。

注意到论文中的一个细节，在一个excel中，使用了一个公共预料库，其中包含了45万个条件。因为我们知道一个excel本身不是编程工具，它们所使用的条件格式比较简单，而不是像程序员编写的代码一样包含算法。尽管收集了45万个条件，但条件本身却不是很多，也不是很复杂。这种条件格式的列举并不会很多，你会发现你很容易找到重复的模板，而这与图像学习相似。

因为我知道学习图片的时候，有人使用了类似方法。比如学习人像图片，在网络上学习一些知名UP主的形象的方法是从视频帧提取的一系列静态图片，得到大量非常相似的图片。为什么这些图片都非常接近呢？因为它们都是同一个人在同一段视频种的形象。（编者注：这种视频帧产生的数据和论文中的低复杂度代码数据有相似之处，所以能够使用Difussion来学习也就可以理解了。）

#### 使用Diffusion学习数据量问题，使用正则表达式提取器检测代码和过滤代码的讨论。

提到了使用Diffusion来学习数据量的问题，并且对此表示赞同。他还谈到了第二个论文，关于Python、Bash和PowerShell的标记，表示这是一个常见的做法，并从GitHub笔记本和Stack Overflow中获取标签信息。然后提到了使用正则表达式提取器来检测代码，并希望能查看论文链接以了解更多。接着对论文的摘要进行了解读，但暂时还不太理解。之后谈到了引用部分，并提到了使用正则表达式的目的是过滤代码。对于正则表达式的有限性进行了讨论，使用正则表达式来提取和检测代码与前面的一个excel中的条件（是低复杂度代码）相呼应。（编者注：因此印证了使用Diffussion训练数据量大、复杂度低的代码是可理解的。）

关于论文的筛选过程和使用的数据来源，作者使用了 Github 和 Stackoverflow 来获取大量的代码，并进行了过滤，选取了相对简单的代码作为训练数据。这种方法符合 diffusion 的特点，并且与之前提到的使用45万个条件格式的 Excel 条件格式类似，说明作者的研究与之前的研究呼应。从这里可以看出，作者相信使用 diffusion 能够训练出有效的结果，并解释了蒋老师的猜想。

在讨论中，有人认同这个观点，并提到之前看到过不同风格的 diffusion 作品，每个模型只能训练一种风格。对于 diffusion 的相关内容，有人表达了不了解的态度，希望在学习之后再进行深入的讨论。因此，讨论的重点回到第一篇论文，即 retrieve anything to argument large number of models。

#### 基于LM反馈的奖励机制用于衡量数据集差异，使用KL散度最小化数据分布差距，通过交叉熵缩小差距并得到高奖励分数。还提到候选句子排序和409兆大小的LLM模型。测试数据及结果未明确。

论文讨论了基于m反馈的奖励机制，这个奖励机制是用来衡量候选数据集和真实数据集之间的差异。其中使用了KL散度来最小化两个数据分布之间的差距。通过交叉熵可以缩小这两个数据分布之间的差距，从而得到更高的奖励分数。此外，论文提到了候选句子的排序，可以根据奖励分数对候选句子进行排序，类似于搜索场景中的重排。论文中使用了409兆大小的LLM模型，类似于BERT模型，但还不清楚他们使用的具体测试数据及测试结果。关于测试数据集是否是通用数据集还需要进一步了解，暂时没有其他同学提到这个问题。

#### 会议结束，讨论总结将在Github提交。

你自己还有什么疑问或者还有什么想讨论的吗？今天大家的问题不多，我们可以把今天的讨论总结发到Github和两篇论文的链接，晚上继续讨论。
